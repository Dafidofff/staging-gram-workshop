<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-07-04T15:05:14+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">blank</title><subtitle>GRaM Workshop
</subtitle><entry><title type="html">Equivariant Neural Fields - continuous representations grounded in geometry</title><link href="http://localhost:4000/blog/2024/equivariant-neural-fields/" rel="alternate" type="text/html" title="Equivariant Neural Fields - continuous representations grounded in geometry" /><published>2024-06-01T00:00:00+02:00</published><updated>2024-06-01T00:00:00+02:00</updated><id>http://localhost:4000/blog/2024/equivariant-neural-fields</id><content type="html" xml:base="http://localhost:4000/blog/2024/equivariant-neural-fields/"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Neural fields (NeFs) <d-cite key="xie2022neural"></d-cite> have emerged as a promising paradigm for representing continuous signals in a variety of domains. 
Recently, they have been used as a continuous alternative for classical discrete signal representations - showing promising results especially in higher dimensional settings where traditional grid-based methods often fall short <d-cite key="dupont2022data"></d-cite>.</p>

<p>A major limitation of NeFs as representation is their lack of interpretibility and preservation of geometric information. In this blog post, we delve into the recent advancements presented in the paper “Grounding Continuous Representations in Geometry: Equivariant Neural Fields” <d-cite key="wessels2024ENF"></d-cite>, and explore how Equivariant Neural Fields (ENFs) enhance the capabilities of NeFs through geometric grounding and equivariance properties. We then elaborate upon their use as a representation by discussing the paper “Space-Time Continuous PDE Forecasting using Equivariant Neural Fields” <d-cite key="knigge2024pde"></d-cite>, which demonstrates the use of ENFs in modelling spatiotemporal dynamics. An important upcomming field of research, in which the geometric grounding of NeFs is crucial.</p>

<h3 id="the-evolution-of-neural-fields">The Evolution of Neural Fields</h3>

<p>Neural fields are functions that map spatial coordinates to feature representations. For instance, a neural field \(f_{\theta}: \mathbb{R}^d \rightarrow \mathbb{R}^c\) can map pixel coordinates \(x\) to RGB values to represent images. These fields are typically parameterized by neural networks, which are optimized to approximate a target signal \(f_\theta\) within a reconstruction task. Although this gives rise to continuous representations, for multiple signals, the weights \(\theta_f\) are optimized separately for each signal \(f\), leading to a lack of shared structure across different signals and the need to train different seperate models.</p>

<p>Neural fields are functions that map spatial coordinates to feature representations. Specifically, a neural field \(f_{\theta}: \mathbb{R}^d \rightarrow \mathbb{R}^c\) can map pixel coordinates \(x\) to RGB values, thereby representing images. These fields are typically parameterized by neural networks and optimized to approximate a target signal \(f_\theta\) within a reconstruction task.</p>

<p>While this approach results in continuous representations, it also presents a significant drawback. For multiple signals, the weights \(\theta_f\) must be optimized separately for each signal \(f\) This leads to a lack of shared structure across different signals and necessitates training separate models for each individual signal.</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/nf-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/nf-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/nf-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/nf.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Neural Fields. when applied to images, a neural field $f_\theta$ maps coordinates $x \in \mathbb{R}^2$ to pixel values $I(x) \in \mathbb{R}^3$.
</div>

<p>Conditional Neural Fields (CNFs) extend this concept of neural fields by introducing a conditioning variable \(z_f\) that modulates the neural field for a specific signal \(f\). This enhancement allows CNFs to effectively represent an entire dataset of signals \(f \in \mathcal{D}\) using a single, shared set of weights \(\theta\) along with a set of unique conditioning variables \(z_f\). Since these representations are signal-specific, they latents can be used as a representation in downstream tasks. This approach has been successful in various tasks, including classification <d-cite key="dupont2022data"></d-cite>, segmentation <d-cite key="zhang20233dshape2vecset"></d-cite>, and even solving partial differential equations<d-cite key="yin2022continuous"></d-cite> <d-cite key="knigge2024pde"></d-cite>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/cnf-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/cnf-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/cnf-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/cnf.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Conditional Neural Fields. Conditional neural fields extend neural fields by introducing a conditioning variable $z$ that modulates the shared base field $f_\theta$.
</div>

<p>However, conventional CNFs often lack geometric interpretability, they are able to capture textures and appearances which is shown by their performance in reconstruction. However, they do struggle to encode explicit geometric information necessary for tasks requiring spatial reasoning. Think for example of simple geometric transformations like rotations or translations, which are not inherently captured by CNFs; it is unclear how these transformations would manifest in the latent space.</p>

<h3 id="introducing-equivariant-neural-fields">Introducing Equivariant Neural Fields</h3>

<p>Equivariant Neural Fields (ENFs) address this limitation by grounding neural field representations in geometry. ENFs use latent point clouds as conditioning variables, where each point is a tuple consisting of a pose and a context vector. This grounding ensures that transformations in the field correspond to transformations in the latent space, a property known as equivariance.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/ENF_latents-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/ENF_latents-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/ENF_latents-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/ENF_latents.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Equivariant Neural Fields parameterize the conditioning variable $z$ as an attributed point-cloud of poses $p_i$ and corresponding context vectors $\mathbf{c}_i$: $z = \{ (p_i, \mathbf{c}_i \}_{i=0}^N$, explicitly grounding the latent space in geometry. 
</div>

<h3 id="key-properties-of-enfs">Key Properties of ENFs</h3>

<ul>
  <li><strong>Equivariance</strong>: If the field transforms, the latent representation transforms accordingly. This property ensures that the latent space preserves geometric patterns, enabling better geometric reasoning.</li>
  <li><strong>Weight Sharing</strong>: ENFs utilize shared weights over similar local patterns, leading to more efficient learning.</li>
  <li><strong>Localized Representations</strong>: The latent point sets in ENFs enable localized cross-attention mechanisms, enhancing interpretability and allowing unique field editing capabilities.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/enf-properties-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/enf-properties-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/enf-properties-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/enf-properties.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Illustration of key properties of Equivariant Neural Fields. ENFs exhibit equivariance by weight-sharing over local patterns through a latent set of poses and context vectors. This enables localized representations and geometric reasoning in the latent space.
</div>

<h3 id="use-of-neural-fields-in-downstream-tasks">Use of Neural Fields in downstream tasks</h3>
<p>As brief interjection, we provide some background on how NeFs are used in downstream tasks. As (a subset of) model NeF parameters are optimized reconstruct specific samples, these parameters may be used as a representation of their corresponding signals. These representations serve as input to downstream models for tasks such as classification, segmentation or even solving partial differential equations.</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/downstream-example-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/downstream-example-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/downstream-example-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/downstream-example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Using NeFs in downstream tasks. For "conventional" NeFs, the weights $\theta_j$ are used as input to a downstream model that can operate on the computational graph of the neural field. For CNFs, the latent vectors $z_j$ are used as representation instead, allowing the use of simple MLPs.
    In ENFs instead the latent point sets $z_j$ are used as input to the downstream model, allowing for preservation of geometric information in the downstream task through the use of equivariant graph models.
</div>

<h2 id="methodology">Methodology</h2>
<p>We now delve into the technical details of ENF, focusing on the key components that enable the model to ground continuous representations in geometry.</p>

<h3 id="requirements-for-equivariant-neural-fields">Requirements for Equivariant Neural Fields</h3>
<p>The equivariance or steerability property of ENFs can be formally defined as:</p>

\[\forall g \in G : f_{\theta}(g^{-1}x, z) = f_{\theta}(x, gz).\]

<p>This property ensures that if the field as a whole transforms, the latent representation will transform in a consistent manner. This is crucial for maintaining geometric coherence in the latent space.
In order reason about the application of a group action on $z$, the authors equip the latent space with a group action by defining $z$ as a set of tuples \((p_i, \mathbf{c}_i)\), where $G$ acts on $z$ by transforming poses \(p_i: gz = \{(gp_i, \mathbf{c}_i)\}_{i=1}^N\).</p>

<p>For a neural field to satisfy the steerability property, the authors show it must be bi-invariant with respect to both coordinates and latents. 
This means that the field $ f_{\theta} $ must remain unchanged under group transformations applied to both the input coordinates and the latent point cloud, i.e.:</p>

\[\forall g \in G: f_\theta(gx, gz) = f_\theta(x, z).\]

<p>This observation is leveraged to define the architecture of ENFs, ensuring that the model is equivariant by design.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/enf-comm-diag-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/enf-comm-diag-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/enf-comm-diag-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/enf-comm-diag.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Commutative diagram illustrating the steerability of ENFs. 
</div>

<h3 id="equivariance-through-bi-invariant-cross-attention">Equivariance through Bi-invariant Cross-Attention</h3>
<p>ENFs utilize a bi-invariant cross-attention mechanism to parametrize the neural fields in order to achieve the aforementioned steerability property.
The cross-attention operation is defined as:
$ f_{\theta}(x, z) = \sum_{i=1}^{N} \text{att}(x, z) v(a(x, p_i), c_i) $
where $ a(x, p_i) $ is an invariant pair-wise attribute that captures the geometric relationship between the coordinate $ x $ and the latent pose $ p_i $, 
ensuring that the cross-attention operation respects the aforementioned bi-invariance condition.</p>

<p>Note that the choice of bi-invariant is related to the choice of group \(G\) and the specific application domain. For example,
in natural images \(G\) may be the group of 2D Euclidean transformations, while in 3D shape representations \(G\) may be the group of 3D rigid transformations,
leading to different choices of bi-invariant \(a(x, p_i)\). For a better understanding of bi-invariant properties we refer to <d-cite key="bekkers2023fast"></d-cite> which shows optimal invariant attributes in terms of expressivity for different groups.</p>

<h3 id="enforcing-locality-in-equivariant-neural-fields">Enforcing Locality in Equivariant Neural Fields</h3>
<p>To enforce locality, ENFs incorporate a Gaussian window into the attention mechanism. 
This ensures that each coordinate receives attention primarily from nearby latents, akin to the localized kernels in convolutional networks. 
This locality improves the interpretability of the latent representations, as specific features can be related to specific latent points $(p_i, \mathbf{c}_i)$.
Moreover, locality also improves parameter-efficiency by allowing for weight sharing over similar patterns.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/enf-summ-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/enf-summ-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/enf-summ-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/enf-summ.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Fitting different signals with ENFs. A field $f_j$ denoting a specific signal is represented by a set of localized latent points $z_j=\{ (p_i, \mathbf{c}_i)\}_{i=1}^N$. In the case of images (left), the latent points are distributed over the image plane. In the case of shapes (right), the latent points are distributed over 3D space.
</div>

<h2 id="experimental-validation">Experimental Validation</h2>
<p>The authors validate the properties of ENFs through various experiments on image and shape datasets, providing 
metrics for reconstruction and downstream classification. Moreover, authors play around with the ENFs latent space
to demonstrate the benefits of having a geometrically grounded latent space. A
separate study by Knigge et al. demonstrates the use of ENFs in modelling spatiotemporal dynamics.</p>

<h3 id="image-and-shape-reconstruction-and-classification">Image and Shape Reconstruction and Classification</h3>
<p>ENFs were evaluated on several image datasets, including CIFAR-10, CelebA, and STL-10. 
The results show that ENFs achieve higher peak signal-to-noise ratio (PSNR) in image reconstruction tasks compared to CNFs. 
This improvement is attributed to the geometric grounding and weight-sharing properties of ENFs.</p>

<table>
  <thead>
    <tr>
      <th><strong>Model</strong></th>
      <th><strong>Symmetry</strong></th>
      <th><strong>Cifar10</strong></th>
      <th><strong>CelebA</strong></th>
      <th><strong>STL-10</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Functa       <d-cite key="dupont2022data"></d-cite></td>
      <td>x</td>
      <td>31.9</td>
      <td>28.0</td>
      <td>20.7</td>
    </tr>
    <tr>
      <td><strong>ENF - abs pos</strong></td>
      <td>x</td>
      <td>31.5</td>
      <td>16.8</td>
      <td>22.8</td>
    </tr>
    <tr>
      <td><strong>ENF - rel pos</strong></td>
      <td>$\mathbb{R}^2$</td>
      <td><strong>34.8</strong></td>
      <td><strong>34.6</strong></td>
      <td><strong>26.8</strong></td>
    </tr>
    <tr>
      <td><strong>ENF - abs rel pos</strong></td>
      <td>SE(2)</td>
      <td>32.8</td>
      <td>32.4</td>
      <td>23.9</td>
    </tr>
    <tr>
      <td><strong>ENF - ponita</strong></td>
      <td>$\rm SE(2)$</td>
      <td>33.9</td>
      <td>32.9</td>
      <td>25.4</td>
    </tr>
  </tbody>
</table>

<div class="caption">
    Reconstruction accuracy for ENFs compared to CNFs on CIFAR-10, CelebA and STL10 for different choices of bi-invariant $a$.
</div>

<p>For classification, the authors used the latent point sets extracted from the trained ENF models. 
The classification accuracy on CIFAR-10 shows a significant improvement over conventional CNFs, 
highlighting the superior representation capabilities of ENFs.</p>

<table>
  <thead>
    <tr>
      <th><strong>Model</strong></th>
      <th><strong>Symmetry</strong></th>
      <th><strong>Cifar10</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Functa <d-cite key="dupont2022data"></d-cite></td>
      <td>x</td>
      <td>68.3</td>
    </tr>
    <tr>
      <td><strong>ENF - abs pos</strong></td>
      <td>x</td>
      <td>68.7</td>
    </tr>
    <tr>
      <td><strong>ENF - rel pos</strong></td>
      <td>$\mathbb{R}^2$</td>
      <td><strong>82.1</strong></td>
    </tr>
    <tr>
      <td><strong>ENF - abs rel pos</strong></td>
      <td>SE(2)</td>
      <td>70.9</td>
    </tr>
    <tr>
      <td><strong>ENF - ponita</strong></td>
      <td>SE(2)</td>
      <td>81.5</td>
    </tr>
  </tbody>
</table>

<div class="caption">
    Classification accuracy for ENFs compared to CNFs on CIFAR-10 for different choices of bi-invariant $a$.
</div>

<p>The authors also tested ENFs on shape datasets using Signed Distance Functions (SDFs). 
The results indicate that ENFs can effectively represent geometric shapes with high fidelity, 
further validating the geometric interpretability of the latent representations.</p>

<table>
  <thead>
    <tr>
      <th><strong>Model</strong></th>
      <th><strong>Reconstruction IoU (voxel)</strong></th>
      <th><strong>Reconstruction IoU (SDF)</strong></th>
      <th><strong>Classification</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Functa <d-cite key="dupont2022data"></d-cite></td>
      <td>99.44</td>
      <td>-</td>
      <td>93.6</td>
    </tr>
    <tr>
      <td><strong>ENF</strong></td>
      <td>-</td>
      <td>55</td>
      <td>89</td>
    </tr>
  </tbody>
</table>

<div class="caption">
    Shape reconstruction and classification metrics for ENFs compared to CNFs on ShapeNet.
</div>

<h3 id="latent-space-editing">Latent Space Editing</h3>
<p>The authors demonstrate the benefits of the geometrically grounded latent space in ENFs by performing latent space editing.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/enf-carduck-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/enf-carduck-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/enf-carduck-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/enf-carduck.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Latent space editing with ENFs. By construction, ENF representations can be stitched together to create new fields. Here, the authors demonstrate the ability to create a "car-duck" by combining the latent representations of reconstructions of a car and a duck.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/interpolation-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/interpolation-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/interpolation-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/interpolation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Localized latent space interpolation. The authors demonstrate the ability to interpolate between two neural fields by interpolating between their latent point sets. This allows for localized editing of the fields.
</div>

<h3 id="spatiotemporal-dynamics-modelling">Spatiotemporal Dynamics Modelling</h3>

<p>Another usecase for ENFs is highlighted in the paper “Space-Time Continuous PDE Forecasting using Equivariant Neural Fields” <d-cite key="knigge2024pde"></d-cite>.
Authors use the ENF as a continuous state representation for solving partial differential equations; learning to forecast dynamics by modelling them with a Neural ODE as a
flow in the latent space of the ENF. Since PDEs are often defined over continuous domains in terms of local differential operators, ENFs are well-suited to model these dynamics, as
they provide localized continuous representations.
This approach allows for symmetry-preserving continuous forecasting of spatiotemporal dynamics,
showing promising results on a variety of PDEs defined over different geometries. An initial state \(\nu_0:\mathcal{X}\rightarrow \mathbb{R}\) representing the current state of the PDE
is fit with a corresponding latent \(z_0\), which is unrolled in latent space with an equivariant graph-based neural ODE \(F_\psi\).</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/fig-enf-pde-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/fig-enf-pde-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/fig-enf-pde-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/fig-enf-pde.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    In Equivariant Neural Fields $f_\theta$, a field $\nu_t$ is represented by a set of latents 
$z^\nu_t = \{(p_{i}^\nu,\mathbf{c}_{i}^\nu)\}_{i=1}^N$ consisting of a pose $p_{i}$ and context vector $\mathbf{c}_{i}$. 
Using meta-learning, the initial latent $z^\nu_0$ is fit in only 3 SGD steps, after which an equivariant neural ODE $F_\psi$ models the solution as a latent flow.
</div>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/shallow-water-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/shallow-water-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/shallow-water-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/shallow-water.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
Due to its continuous nature, the ENF forecasting model is able to natively handle zero-shot super-resolution, as demonstrated on the shallow water equations. 
Top: low resolution test sample at train resolution. Middle: high resolution test sample at test resolution. Bottom: ENF forecast at test resolution. 
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/internally-heated-convection-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/internally-heated-convection-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/internally-heated-convection-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/internally-heated-convection.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
Because of its geometric grounding, the model is able to handle complicated geometries, as demonstrated on internally heated convection equations in the ball.
</div>

<iframe src="/assets/plotly/2024-06-15-equivariant-neural-fields/navier-stokes.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
<div class="caption">
    The authors demonstrate the use of ENFs in modelling spatiotemporal dynamics by solving the Navier-Stokes 
equations over a 2D domain with periodic boundary conditions. The ENF respects the corresponding translational symmetries. Left: ground truth dynamics. Middle: ENF forecast. Right: Absolute forecast error. A test-sample is visualized, i.e. the model is unrolled from the initial state $\nu_0$. During training the model is supervised for 10 timesteps.
</div>

<h2 id="conclusion">Conclusion</h2>

<p>Equivariant Neural Fields leverage geometric grounding and equivariance properties to provide continuous signal representations preserving geometric information.
This approach opens up new possibilities for tasks that require geometric reasoning and localized representations, such as image and shape analysis, and shows promising results in 
forecasting spatiotemporal dynamics.</p>

<p>This blog post has explored the foundational concepts and the significant advancements brought forward by Equivariant Neural Fields. By grounding neural fields in geometry and incorporating equivariance properties, ENFs pave the way for more robust and interpretable continuous signal representations. As research in this area progresses, we can expect further innovations that leverage the geometric and localized nature of these fields, unlocking new potentials across diverse applications.</p>]]></content><author><name>David R. Wessels*</name></author><category term="equivariance" /><category term="geometry" /><category term="neural-fields" /><summary type="html"><![CDATA[An intro to geometry-grounded continuous signal representations and their use in modelling spatio-temporal dynamics.]]></summary></entry><entry><title type="html">a distill-style blog post</title><link href="http://localhost:4000/blog/2021/distill/" rel="alternate" type="text/html" title="a distill-style blog post" /><published>2021-05-22T00:00:00+02:00</published><updated>2021-05-22T00:00:00+02:00</updated><id>http://localhost:4000/blog/2021/distill</id><content type="html" xml:base="http://localhost:4000/blog/2021/distill/"><![CDATA[<h2 id="equations">Equations</h2>

<p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine.
You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>.
If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p>

<p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph.
Here is an example:</p>

\[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\]

<p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p>

<hr />

<h2 id="citations">Citations</h2>

<p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.</p>

<p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.</p>

<p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p>

<hr />

<h2 id="footnotes">Footnotes</h2>

<p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p>

<hr />

<h2 id="code-blocks">Code Blocks</h2>

<p>Syntax highlighting is provided within <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> tags.
An example of inline code snippets: <code class="language-plaintext highlighter-rouge">&lt;d-code language="html"&gt;let x = 10;&lt;/d-code&gt;</code>.
For larger blocks of code, add a <code class="language-plaintext highlighter-rouge">block</code> attribute:</p>

<d-code block="" language="javascript">
  var x = 25;
  function(x) {
    return x * x;
  }
</d-code>

<p><strong>Note:</strong> <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> blocks do not look good in the dark mode.
You can always use the default code-highlight using the <code class="language-plaintext highlighter-rouge">highlight</code> liquid tag:</p>

<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="kd">var</span> <span class="nx">x</span> <span class="o">=</span> <span class="mi">25</span><span class="p">;</span>
<span class="kd">function</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span> <span class="p">{</span>
<span class="k">return</span> <span class="nx">x</span> <span class="err">\</span><span class="o">*</span> <span class="nx">x</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>

<hr />

<h2 id="interactive-plots">Interactive Plots</h2>

<p>You can add interative plots using plotly + iframes :framed_picture:</p>

<div class="l-page">
  <iframe src="/assets/plotly/distill-template/demo.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

<p>The plot must be generated separately and saved into an HTML file.
To generate the plot that you see above, you can use the following code snippet:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span>
<span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span>
<span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
<span class="n">df</span><span class="p">,</span>
<span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span>
<span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span>
<span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span>
<span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span>
<span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">assets/distill-template/plotly/demo.html</span><span class="sh">'</span><span class="p">)</span></code></pre></figure>

<hr />

<h2 id="details-boxes">Details boxes</h2>

<p>Details boxes are collapsible boxes which hide additional information from the user. They can be added with the <code class="language-plaintext highlighter-rouge">details</code> liquid tag:</p>

<details><summary>Click here to know more</summary>
<p>Additional details, where math \(2x - 1\) and <code class="language-plaintext highlighter-rouge">code</code> is rendered correctly.</p>
</details>

<hr />

<h2 id="layouts">Layouts</h2>

<p>The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p>

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

<p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p>

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

<p>All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:</p>

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

<p>Occasionally you’ll want to use the full browser width.
For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>.
You can also inset the element a little from the edge of the browser by using the inset variant.</p>

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

<p>The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code> sized text except on mobile screen sizes.</p>

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>

<hr />

<h2 id="images">Images</h2>

<p>This is an example post with image galleries.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/distill-template/9-480.webp 480w,/assets/img/distill-template/9-800.webp 800w,/assets/img/distill-template/9-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/distill-template/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/distill-template/7-480.webp 480w,/assets/img/distill-template/7-800.webp 800w,/assets/img/distill-template/7-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/distill-template/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all.
</div>

<p>Images can be made zoomable.
Simply add <code class="language-plaintext highlighter-rouge">data-zoomable</code> to <code class="language-plaintext highlighter-rouge">&lt;img&gt;</code> tags that you want to make zoomable.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/distill-template/8-480.webp 480w,/assets/img/distill-template/8-800.webp 800w,/assets/img/distill-template/8-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/distill-template/8.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/distill-template/10-480.webp 480w,/assets/img/distill-template/10-800.webp 800w,/assets/img/distill-template/10-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/distill-template/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<p>The rest of the images in this post are all zoomable, arranged into different mini-galleries.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/distill-template/11-480.webp 480w,/assets/img/distill-template/11-800.webp 800w,/assets/img/distill-template/11-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/distill-template/11.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/distill-template/12-480.webp 480w,/assets/img/distill-template/12-800.webp 800w,/assets/img/distill-template/12-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/distill-template/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/distill-template/7-480.webp 480w,/assets/img/distill-template/7-800.webp 800w,/assets/img/distill-template/7-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/distill-template/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<hr />

<h2 id="other-typography">Other Typography?</h2>

<p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p>

<p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p>

<p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p>

<p>Strikethrough uses two tildes. <del>Scratch this.</del></p>

<ol>
  <li>First ordered list item</li>
  <li>Another item
⋅⋅* Unordered sub-list.</li>
  <li>Actual numbers don’t matter, just that it’s a number
⋅⋅1. Ordered sub-list</li>
  <li>And another item.</li>
</ol>

<p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p>

<p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅
⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅
⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)</p>

<ul>
  <li>
    <p>Unordered list can use asterisks</p>
  </li>
  <li>
    <p>Or minuses</p>
  </li>
  <li>
    <p>Or pluses</p>
  </li>
</ul>

<p><a href="https://www.google.com">I’m an inline-style link</a></p>

<p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p>

<p><a href="https://www.mozilla.org">I’m a reference-style link</a></p>

<p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p>

<p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p>

<p>URLs and URLs in angle brackets will automatically get turned into links.
http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes
example.com (but not on Github, for example).</p>

<p>Some text to show that the reference links can follow later.</p>

<p>Here’s our logo (hover to see the title text):</p>

<p>Inline-style:
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1" /></p>

<p>Reference-style:
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2" /></p>

<p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="k">print</span> <span class="n">s</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div>

<p>Colons can be used to align columns.</p>

<table>
  <thead>
    <tr>
      <th>Tables</th>
      <th style="text-align: center">Are</th>
      <th style="text-align: right">Cool</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>col 3 is</td>
      <td style="text-align: center">right-aligned</td>
      <td style="text-align: right">$1600</td>
    </tr>
    <tr>
      <td>col 2 is</td>
      <td style="text-align: center">centered</td>
      <td style="text-align: right">$12</td>
    </tr>
    <tr>
      <td>zebra stripes</td>
      <td style="text-align: center">are neat</td>
      <td style="text-align: right">$1</td>
    </tr>
  </tbody>
</table>

<p>There must be at least 3 dashes separating each header cell.
The outer pipes (|) are optional, and you don’t need to make the
raw Markdown line up prettily. You can also use inline Markdown.</p>

<table>
  <thead>
    <tr>
      <th>Markdown</th>
      <th>Less</th>
      <th>Pretty</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>Still</em></td>
      <td><code class="language-plaintext highlighter-rouge">renders</code></td>
      <td><strong>nicely</strong></td>
    </tr>
    <tr>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>Blockquotes are very handy in email to emulate reply text.
This line is part of the same quote.</p>
</blockquote>

<p>Quote break.</p>

<blockquote>
  <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p>
</blockquote>

<p>Here’s a line for us to start with.</p>

<p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p>

<p>This line is also a separate paragraph, but…
This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><category term="distill" /><category term="formatting" /><summary type="html"><![CDATA[an example of a distill-style blog post and main elements]]></summary></entry></feed>